syntax = "proto3";
option optimize_for = LITE_RUNTIME;
// all the matrix are stored in row-major order,
// plz see https://en.wikipedia.org/wiki/Row-_and_column-major_order for details

// the definition of "Multi-Head Attention", "Scaled Dot-Product Attention" and
// "Feed-Forward Networks"
// plz see https://arxiv.org/abs/1706.03762 for details

message QuantVitEncoderLayer {
  // layer norm before "Multi-Head Attention"
  repeated float multihead_norm_scale = 1; // [hidden_size]
  repeated float multihead_norm_bias = 2; // [hidden_size]

  // "Multi-Head Attention" linearly project weights kernel for query, key,
  // value,
  // before "Scaled Dot-Product Attention, with shape (hidden_size,
  // hidden_size*3)
  // is built by numpy.concatenate((query_kernel, key_kernel, value_kernel),
  // axis=1)
  // perform numpy.dot(input, multihead_project_kernel_qkv) will get the [query,
  // key, value] of
  // "Scaled Dot-Product Attention"
  bytes multihead_project_kernel_qkv = 3; // [hidden_size, 3, hidden_size]
  repeated float multihead_project_bias_qkv = 4; // [3, hidden_size]
  bytes multihead_project_kernel_output = 5; // [hidden_size, hidden_size]
  repeated float multihead_project_bias_output = 6; // [hidden_size]

  // layer norm before "Feed-Forward Networks"
  repeated float ffn_norm_scale = 7; // [hidden_size]
  repeated float ffn_norm_bias = 8; // [hidden_size]

  // "Feed-Forward Networks"
  bytes ffn_first_kernel = 9; // [hidden_size, inner_size]
  repeated float ffn_first_bias = 10; // [inner_size]
  bytes ffn_second_kernel = 11; // [inner_size, hidden_size]
  repeated float ffn_second_bias = 12; // [hidden_size]

  // clip max
  float multihead_project_kernel_qkv_clip_max = 13;
  float multihead_project_kernel_output_clip_max = 14;
  float ffn_first_kernel_clip_max = 15;
  float ffn_second_kernel_clip_max = 16;
  float multihead_ln_clip_max = 17;
  float multihead_project_output_clip_max = 18;
  float ffn_ln_clip_max = 19;
  float ffn_first_act_clip_max = 20;
  float multihead_qkv_dense_clip_max = 21;
  float multihead_output_dense_clip_max = 22;
  float ffn_first_output_clip_max = 23;
}

message QuantVitEmbeddingLayer {
  // weight and bias of convolution in patch embedding
  repeated float conv_weight = 1; // [hidden_size, channel_input, patch_size, patch_size]
  repeated float conv_bias = 2; // [hidden_size]
  // learnable position embedding
  repeated float position_embedding = 3; // [max_seq_len, hidden_size]
  repeated float cls_embedding = 4; // [hidden_size]
  // the last layer_norm of encoder,
  // only for pre layer norm,
  repeated float norm_scale = 5; // [hidden_size]
  repeated float norm_bias = 6; // [hidden_size]
}

message QuantVitModelConf {
  int32 head_num = 1;
  bool use_gelu = 2; // use gelu for activation otherwise relu
  int32 image_size = 3; // width of input image
  int32 patch_size = 4; //width of patch and convolution kernel
  bool is_post_ln = 5; // Pre-LN or Post-LN
}

message QuantVit {
  QuantVitEmbeddingLayer src_embedding = 1;
  repeated QuantVitEncoderLayer encoder_stack = 2;
  QuantVitModelConf model_conf = 3;
}
