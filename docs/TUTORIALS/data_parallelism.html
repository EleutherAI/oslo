

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Data Parallelism Tutorial &#8212; OSLO  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'TUTORIALS/data_parallelism';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Zero redundancy optimizer Tutorial" href="zero_redundancy_optimizer.html" />
    <link rel="prev" title="Concept of Tensor Model Parallelism" href="../CONCEPTS/tensor_model_parallelism.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">OSLO  documentation</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">CONCEPTS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../CONCEPTS/parallel_context.html">Concept of Parallel Context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CONCEPTS/data_parallelism.html">Concept of Data Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CONCEPTS/tensor_model_parallelism.html">Concept of Tensor Model Parallelism</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TUTORIALS</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Data Parallelism Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="zero_redundancy_optimizer.html">Zero redundancy optimizer Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_model_parallelism.html">Tensor Model Parallelism Tutorial</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/TUTORIALS/data_parallelism.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Data Parallelism Tutorial</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-contents">Table of contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-launcher">0. Distributed Launcher</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">1. Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-some-variables">1.1. Initialize some variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model-optimizer-and-tokenizer">1.2. Create model, optimizer and tokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelize-the-model">1.3. Parallelize the model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-dataset-and-create-dataloader">1.4. Load dataset and create dataloader</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-training-as-usual">1.5. Do training as usual</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-multi-node-training">Appendix. Multi-node Training</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="data-parallelism-tutorial">
<h1>Data Parallelism Tutorial<a class="headerlink" href="#data-parallelism-tutorial" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>Authors: Jinwon Kim</p></li>
</ul>
<p><strong>Data Parallelism</strong>
is a widely-used technique for training deep learning models in parallel. It involves distributing the training data across multiple processing units, such as GPUs, each of which has a copy of the model parameters. The data is divided into subsets, and each unit independently computes the gradients for its subset. The gradients are then aggregated to update the model parameters. This approach enables efficient parallelization of the training process and can accelerate the training of deep learning models on large datasets.</p>
<section id="table-of-contents">
<h2>Table of contents<a class="headerlink" href="#table-of-contents" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#"><span class="xref myst">Data Parallelism Tutorial</span></a></p>
<ul>
<li><p><a class="reference internal" href="#table-of-contents"><span class="xref myst">Table of contents</span></a></p></li>
<li><p><a class="reference internal" href="#0-distributed-launcher"><span class="xref myst">0. Distributed Launcher</span></a></p></li>
<li><p><a class="reference internal" href="#1-training"><span class="xref myst">1. Training</span></a></p>
<ul>
<li><p><a class="reference internal" href="#21-initialize-some-variables"><span class="xref myst">1.1. Initialize some variables</span></a></p></li>
<li><p><a class="reference internal" href="#22-create-model-optimizer-and-tokenizer"><span class="xref myst">1.2. Create model, optimizer and tokenizer</span></a></p></li>
<li><p><a class="reference internal" href="#23-parallelize-the-model"><span class="xref myst">1.3. Parallelize the model</span></a></p></li>
<li><p><a class="reference internal" href="#24-load-dataset-and-create-dataloader"><span class="xref myst">1.4. Load dataset and create dataloader</span></a></p></li>
<li><p><a class="reference internal" href="#25-do-training-as-usual"><span class="xref myst">1.5. Do training as usual</span></a></p></li>
<li><p><a class="reference internal" href="#26-save-the-parallelized-model"><span class="xref myst">1.6. Save the parallelized model</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#appendix-multi-node-training"><span class="xref myst">Appendix. Multi-node Training</span></a></p></li>
</ul>
</li>
</ul>
</section>
<section id="distributed-launcher">
<h2>0. Distributed Launcher<a class="headerlink" href="#distributed-launcher" title="Permalink to this heading">#</a></h2>
<p>This tutorial must be launched using distributed launcher.</p>
<p>If you have 4 GPUs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torchrun --nproc_per_node<span class="o">=</span><span class="m">4</span> YOUR_SCRIPT.py
</pre></div>
</div>
<p>If you installed Slurm in your environments, the following works the same.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun --num_gpus<span class="o">=</span><span class="m">4</span> YOUR_SCRIPT.py
</pre></div>
</div>
<p>For more information of the distributed launchers, refer to:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">Pytorch documents</a></p></li>
<li><p><a class="reference external" href="https://www.deepspeed.ai/getting-started/#launching-deepspeed-training">DeepSpeed documents</a></p></li>
</ul>
</section>
<section id="training">
<h2>1. Training<a class="headerlink" href="#training" title="Permalink to this heading">#</a></h2>
<p>How to use the data parallelism for training?</p>
<section id="initialize-some-variables">
<h3>1.1. Initialize some variables<a class="headerlink" href="#initialize-some-variables" title="Permalink to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">SEQ_LEN</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">SAVE_INTERVAL</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">TRAIN_STEP</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
</section>
<section id="create-model-optimizer-and-tokenizer">
<h3>1.2. Create model, optimizer and tokenizer<a class="headerlink" href="#create-model-optimizer-and-tokenizer" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="c1"># Add pad token for batch training because GPT2 tokenizer doesn&#39;t have pad token.</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</section>
<section id="parallelize-the-model">
<h3>1.3. Parallelize the model<a class="headerlink" href="#parallelize-the-model" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># model = defined in section 1.2</span>

<span class="kn">from</span> <span class="nn">oslo</span> <span class="kn">import</span> <span class="n">ParallelContext</span><span class="p">,</span> <span class="n">ParallelMode</span>
<span class="kn">from</span> <span class="nn">oslo.torch.nn.parallel.data_parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="n">dp_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">parallel_context</span> <span class="o">=</span> <span class="n">ParallelContext</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span>
    <span class="n">data_parallel_size</span><span class="o">=</span><span class="n">dp_size</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parallel_context</span><span class="p">)</span>
<span class="n">oslo</span><span class="o">.</span><span class="n">ready</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parallel_context</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="load-dataset-and-create-dataloader">
<h3>1.4. Load dataset and create dataloader<a class="headerlink" href="#load-dataset-and-create-dataloader" title="Permalink to this heading">#</a></h3>
<p>In this tutorial, We’re going to use <code class="docutils literal notranslate"><span class="pre">datasets</span></code> library of Hugging Face.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">DistributedSampler</span>

    <span class="n">rank</span> <span class="o">=</span> <span class="n">parallel_context</span><span class="o">.</span><span class="n">get_local_rank</span><span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA</span><span class="p">)</span>

    <span class="n">datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;squad&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
    <span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">_</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">datasets</span><span class="p">[:</span> <span class="n">TRAIN_STEP</span> <span class="o">*</span> <span class="n">BATCH_SIZE</span><span class="p">]]</span>
    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
        <span class="n">datasets</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">LOCAL_WORLD_SIZE</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span>
    <span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">datasets</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="do-training-as-usual">
<h3>1.5. Do training as usual<a class="headerlink" href="#do-training-as-usual" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Note: Please do not use <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, the standard way to use is <code class="docutils literal notranslate"><span class="pre">model.zero_grad()</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Make batch</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">SEQ_LEN</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

    <span class="c1"># Forward-Backward-Step</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">loss</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="appendix-multi-node-training">
<h2>Appendix. Multi-node Training<a class="headerlink" href="#appendix-multi-node-training" title="Permalink to this heading">#</a></h2>
<p>There are three types of training methods are supported by oslo.</p>
<ol class="arabic">
<li><p>torch distributed ( torchrun, recommended )</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Node #1</span>
torchrun --nnodes<span class="o">=</span><span class="m">2</span> --node_rank<span class="o">=</span><span class="m">0</span> --nproc_per_node<span class="o">=</span><span class="m">4</span> --master_addr<span class="o">=</span><span class="si">${</span><span class="nv">YOUR_NODE_ADDRESS</span><span class="si">}</span> --master_port<span class="o">=</span><span class="si">${</span><span class="nv">PORT</span><span class="si">}</span> YOUR_SCRIPT.py

<span class="c1"># Node #2</span>
torchrun --nnodes<span class="o">=</span><span class="m">2</span> --node_rank<span class="o">=</span><span class="m">1</span> --nproc_per_node<span class="o">=</span><span class="m">4</span> --master_addr<span class="o">=</span><span class="si">${</span><span class="nv">YOUR_NODE_ADDRESS</span><span class="si">}</span> --master_port<span class="o">=</span><span class="si">${</span><span class="nv">PORT</span><span class="si">}</span> YOUR_SCRIPT.py
</pre></div>
</div>
</li>
<li><p>Slurm : Slurm using SBATCH file, and then running sbatch sbatch_file.sh command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=${JOBNAME}</span>
<span class="c1">#SBATCH --partition=gpu</span>
<span class="c1">#SBATCH --time=infinite</span>

<span class="c1">### e.g. request 8 nodes with 8 gpu each, totally 64 gpus (WORLD_SIZE==64)</span>
<span class="c1">### Note: --gres=gpu:x should equal to ntasks-per-node</span>
<span class="c1">#SBATCH --nodes=4</span>
<span class="c1">#SBATCH --ntasks-per-node=4</span>
<span class="c1">#SBATCH --cpus-per-task=6</span>
<span class="c1">#SBATCH --gres=gpu:4             # number of gpus per node</span>
<span class="c1">#SBATCH --mem=64gb</span>

<span class="nb">export</span> <span class="nv">HOSTNAMES</span><span class="o">=</span><span class="sb">`</span>scontrol show hostnames <span class="s2">&quot;</span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span><span class="sb">`</span>
<span class="nb">export</span> <span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>scontrol show hostnames <span class="s2">&quot;</span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span> <span class="p">|</span> head -n <span class="m">1</span><span class="k">)</span>
<span class="nb">export</span> <span class="nv">MASTER_PORT</span><span class="o">=</span><span class="si">${</span><span class="nv">PORT</span><span class="si">}</span>
<span class="nb">export</span> <span class="nv">COUNT_NODE</span><span class="o">=</span><span class="sb">`</span>scontrol show hostnames <span class="s2">&quot;</span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span> <span class="p">|</span> wc -l<span class="sb">`</span>

python YOUR_SCRIPT.py
</pre></div>
</div>
<p>And then, run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch sbatch_file.py
</pre></div>
</div>
</li>
</ol>
</section>
</section>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../CONCEPTS/tensor_model_parallelism.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Concept of Tensor Model Parallelism</p>
      </div>
    </a>
    <a class="right-next"
       href="zero_redundancy_optimizer.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Zero redundancy optimizer Tutorial</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-contents">Table of contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-launcher">0. Distributed Launcher</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">1. Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-some-variables">1.1. Initialize some variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model-optimizer-and-tokenizer">1.2. Create model, optimizer and tokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelize-the-model">1.3. Parallelize the model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-dataset-and-create-dataloader">1.4. Load dataset and create dataloader</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-training-as-usual">1.5. Do training as usual</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-multi-node-training">Appendix. Multi-node Training</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By EleutherAI
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, EleutherAI.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>