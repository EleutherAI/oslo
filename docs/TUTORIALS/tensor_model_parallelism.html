
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Tensor Model Parallelism Tutorial &#8212; OSLO  documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Concept of Tensor Model Parallelism" href="../CONCEPTS/tensor_model_parallelism.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">OSLO  documentation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  CONCEPTS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../CONCEPTS/parallel_context.html">
   Concept of Parallel Context
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CONCEPTS/tensor_model_parallelism.html">
   Concept of Tensor Model Parallelism
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  TUTORIALS
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Tensor Model Parallelism Tutorial
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/TUTORIALS/tensor_model_parallelism.md.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#table-of-contents">
   Table of contents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distributed-launcher">
   0. Distributed Launcher
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   1. Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-model-and-tokenizer">
     1.1. Create model and tokenizer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallelize-the-model">
     1.2. Parallelize the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensor-parallel-algorithms">
     1.2.1 Tensor Parallel Algorithms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-inference-as-usual">
     1.3. Do inference as usual
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   2. Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialize-some-variables">
     2.1. Initialize some variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-model-optimizer-and-tokenizer">
     2.2. Create model, optimizer and tokenizer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.3. Parallelize the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-dataset-and-create-dataloader">
     2.4. Load dataset and create dataloader
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-training-as-usual">
     2.5. Do training as usual
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-the-parallelized-model">
     2.6. Save the parallelized model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#merging-checkpoints">
       2.6.1. Merging Checkpoints
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-multi-node-training">
   Appendix. Multi-node Training
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Tensor Model Parallelism Tutorial</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#table-of-contents">
   Table of contents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distributed-launcher">
   0. Distributed Launcher
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   1. Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-model-and-tokenizer">
     1.1. Create model and tokenizer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallelize-the-model">
     1.2. Parallelize the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensor-parallel-algorithms">
     1.2.1 Tensor Parallel Algorithms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-inference-as-usual">
     1.3. Do inference as usual
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   2. Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialize-some-variables">
     2.1. Initialize some variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-model-optimizer-and-tokenizer">
     2.2. Create model, optimizer and tokenizer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.3. Parallelize the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-dataset-and-create-dataloader">
     2.4. Load dataset and create dataloader
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-training-as-usual">
     2.5. Do training as usual
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-the-parallelized-model">
     2.6. Save the parallelized model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#merging-checkpoints">
       2.6.1. Merging Checkpoints
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-multi-node-training">
   Appendix. Multi-node Training
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="tensor-model-parallelism-tutorial">
<h1>Tensor Model Parallelism Tutorial<a class="headerlink" href="#tensor-model-parallelism-tutorial" title="Permalink to this heading"></a></h1>
<ul class="simple">
<li><p>Authors: Kichang Yang, Kevin Ko</p></li>
</ul>
<p><img alt="260461C3-EA3B-405C-9B34-05BA3C781161.png" src="../_images/260461C3-EA3B-405C-9B34-05BA3C781161.png" /></p>
<p><strong>Tensor Model Parallelism</strong>
makes it possible to train larger models by partitioning the parameter tensors into multiple dimensions. We also support 2D, 2.5D, and 3D tensor partitioning which make tensor parallel training more efficient unlike Megatron-LM which simply splits parameters into single dimensions such as rows and columns.</p>
<section id="table-of-contents">
<h2>Table of contents<a class="headerlink" href="#table-of-contents" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><span class="xref myst">Tensor Model Parallelism Tutorial</span></p>
<ul>
<li><p><span class="xref myst">Table of contents</span></p></li>
<li><p><span class="xref myst">0. Distributed Launcher</span></p></li>
<li><p><span class="xref myst">1. Inference</span></p>
<ul>
<li><p><span class="xref myst">1.1. Create model and tokenizer</span></p></li>
<li><p><span class="xref myst">1.2. Parallelize the model</span></p></li>
<li><p><span class="xref myst">1.2.1 Tensor Parallel Algorithms</span></p></li>
<li><p><span class="xref myst">1.3. Do inference as usual</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">2. Training</span></p>
<ul>
<li><p><span class="xref myst">2.1. Initialize some variables</span></p></li>
<li><p><span class="xref myst">2.2. Create model, optimizer and tokenizer</span></p></li>
<li><p><span class="xref myst">2.3. Parallelize the model</span></p></li>
<li><p><span class="xref myst">2.4. Load dataset and create dataloader</span></p></li>
<li><p><span class="xref myst">2.5. Do training as usual</span></p></li>
<li><p><span class="xref myst">2.6. Save the parallelized model</span></p>
<ul>
<li><p><span class="xref myst">2.6.1. Merging Checkpoints</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><span class="xref myst">Appendix. Multi-node Training</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="distributed-launcher">
<h2>0. Distributed Launcher<a class="headerlink" href="#distributed-launcher" title="Permalink to this heading"></a></h2>
<p>This tutorial must be launched using distributed launcher.</p>
<p>If you have 4 GPUs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torchrun --nproc_per_node<span class="o">=</span><span class="m">4</span> YOUR_SCRIPT.py
</pre></div>
</div>
<p>If you installed Slurm in your environments, the following works the same.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun --num_gpus<span class="o">=</span><span class="m">4</span> YOUR_SCRIPT.py
</pre></div>
</div>
<p>For more information of the distributed launchers, refer to:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">Pytorch documents</a></p></li>
<li><p><a class="reference external" href="https://www.deepspeed.ai/getting-started/#launching-deepspeed-training">DeepSpeed documents</a></p></li>
</ul>
</section>
<section id="inference">
<h2>1. Inference<a class="headerlink" href="#inference" title="Permalink to this heading"></a></h2>
<p>How to use the tensor model parallelism for inference?</p>
<section id="create-model-and-tokenizer">
<h3>1.1. Create model and tokenizer<a class="headerlink" href="#create-model-and-tokenizer" title="Permalink to this heading"></a></h3>
<p>Warning : model must be assigned in cpu, <strong>not</strong> <strong>CUDA</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parallelize-the-model">
<h3>1.2. Parallelize the model<a class="headerlink" href="#parallelize-the-model" title="Permalink to this heading"></a></h3>
<p>You can parallelize the model you defined is only 2 steps required.</p>
<p>Just making define <code class="docutils literal notranslate"><span class="pre">parallel_context</span></code> , and call <code class="docutils literal notranslate"><span class="pre">oslo.ready</span></code> function.</p>
<p><code class="docutils literal notranslate"><span class="pre">parallel_context</span></code> Is the variable for define the method how to parallelize to oslo.</p>
<p>Here is some explain about arguments to parallel_context.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_parallel_size</span></code> must be same or smaller than total num of gpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_parallel_size</span></code> must be power of 2. (e.g. 2, 4, 8, 16, …)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_parallel_size</span></code> must be positive number.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_parallel_mode</span></code> support 4 types of tensor parallel algorithm. For more details, see section 1.2.1</p>
<ul>
<li><p>TENSOR_1D : same as megatronLM</p></li>
<li><p>TENSOR_2D : Using summa algorithm.</p></li>
<li><p>TENSOR_2p5D : Using 2.5d summa algorithm ( much effective in Communication costs between layers. )</p></li>
<li><p>TENSOR_3D : Using cubic-3d algorithm</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_parallel_depth</span></code> must be If the mode is…</p>
<ul>
<li><p>2p5D : same or lower than <code class="docutils literal notranslate"><span class="pre">tensor_parallel_size</span></code></p></li>
<li><p>Not 2p5D : must be 1.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">pipeline_parallel_size</span></code> must be 1 if you want to use <code class="docutils literal notranslate"><span class="pre">tensor_parallel</span></code> algorithm ( mixing PP and PP will be supported in later version.)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oslo</span>
<span class="kn">from</span> <span class="nn">oslo</span> <span class="kn">import</span> <span class="n">ParallelContext</span>
<span class="kn">from</span> <span class="nn">oslo.torch.nn.parallel</span> <span class="kn">import</span> <span class="n">TensorParallel</span>

<span class="n">tp_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">tp_depth</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">parallel_context</span> <span class="o">=</span> <span class="n">ParallelContext</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span>
    <span class="n">data_parallel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">pipeline_parallel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">tp_size</span><span class="p">,</span>
    <span class="n">tensor_parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">TENSOR_1D</span><span class="p">,</span>
    <span class="n">tensor_parallel_depth</span><span class="o">=</span><span class="n">tp_depth</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TensorParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parallel_context</span><span class="p">)</span>
<span class="n">oslo</span><span class="o">.</span><span class="n">ready</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parallel_context</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="tensor-parallel-algorithms">
<h3>1.2.1 Tensor Parallel Algorithms<a class="headerlink" href="#tensor-parallel-algorithms" title="Permalink to this heading"></a></h3>
<p>You can find the details of the algorithms in the CONCEPTS section.
see here:</p>
<ul class="simple">
<li><p><span class="xref myst">Concept of Tensor Parallel Algorithms</span></p></li>
</ul>
</section>
<section id="do-inference-as-usual">
<h3>1.3. Do inference as usual<a class="headerlink" href="#do-inference-as-usual" title="Permalink to this heading"></a></h3>
<p>This is an example of text generation.
In addition to this, it can be used in various tasks such as sequence classification or masked lm.
Likewise, you can write the code as usual.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;I don&#39;t want a lot for Christmas. There is just one thing&quot;</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">I</span> <span class="n">don</span><span class="s1">&#39;t want a lot for Christmas. There is just one thing I want to ...</span>
</pre></div>
</div>
</section>
</section>
<section id="training">
<h2>2. Training<a class="headerlink" href="#training" title="Permalink to this heading"></a></h2>
<p>How to use the tensor model parallelism for training?</p>
<section id="initialize-some-variables">
<h3>2.1. Initialize some variables<a class="headerlink" href="#initialize-some-variables" title="Permalink to this heading"></a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">SEQ_LEN</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">SAVE_INTERVAL</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">TRAIN_STEP</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
</section>
<section id="create-model-optimizer-and-tokenizer">
<h3>2.2. Create model, optimizer and tokenizer<a class="headerlink" href="#create-model-optimizer-and-tokenizer" title="Permalink to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="c1"># Add pad token for batch training because GPT2 tokenizer doesn&#39;t have pad token.</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>2.3. Parallelize the model<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># model = defined in section 2.2</span>

<span class="kn">from</span> <span class="nn">oslo</span> <span class="kn">import</span> <span class="n">ParallelContext</span>
<span class="kn">from</span> <span class="nn">oslo.torch.nn.parallel</span> <span class="kn">import</span> <span class="n">TensorParallel</span>

<span class="n">tp_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">tp_depth</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">parallel_context</span> <span class="o">=</span> <span class="n">ParallelContext</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span>
    <span class="n">data_parallel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">pipeline_parallel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">tp_size</span><span class="p">,</span>
    <span class="n">tensor_parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">TENSOR_1D</span><span class="p">,</span>
    <span class="n">tensor_parallel_depth</span><span class="o">=</span><span class="n">tp_depth</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TensorParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parallel_context</span><span class="p">)</span>
<span class="n">oslo</span><span class="o">.</span><span class="n">ready</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parallel_context</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="load-dataset-and-create-dataloader">
<h3>2.4. Load dataset and create dataloader<a class="headerlink" href="#load-dataset-and-create-dataloader" title="Permalink to this heading"></a></h3>
<p>In this tutorial, We’re going to use <code class="docutils literal notranslate"><span class="pre">datasets</span></code> library of Hugging Face.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;squad&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
<span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">_</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">datasets</span><span class="p">[:</span> <span class="n">TRAIN_STEP</span> <span class="o">*</span> <span class="n">BATCH_SIZE</span><span class="p">]]</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">datasets</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="do-training-as-usual">
<h3>2.5. Do training as usual<a class="headerlink" href="#do-training-as-usual" title="Permalink to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Make batch</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">SEQ_LEN</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

    <span class="c1"># Forward-Backward-Step</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">loss</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="save-the-parallelized-model">
<h3>2.6. Save the parallelized model<a class="headerlink" href="#save-the-parallelized-model" title="Permalink to this heading"></a></h3>
<p>We support <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code>  method, and this is similar with <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained</a> in the Transformers.
So, it can be used with the same argument with <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained</a>.
Then, the checkpoints like <code class="docutils literal notranslate"><span class="pre">pytorch_model_tp_${TP_RANK}_pp_${PP_RANK}_ep_${EP_RANK}.bin</span></code> will be saved in your local path.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the parallelized model using `save_pretrained`</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="o">=</span><span class="s2">&quot;./parallel_ckpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="merging-checkpoints">
<h4>2.6.1. Merging Checkpoints<a class="headerlink" href="#merging-checkpoints" title="Permalink to this heading"></a></h4>
<p>If you want save your models with merged status, you just only pass one more arguments <code class="docutils literal notranslate"><span class="pre">merge_checkpoints=True</span></code> to <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code> function.</p>
<p>Here is the modified code in section 2.6 for save checkpoints to merged version.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the merged model using `save_pretrained`</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span>
    <span class="n">save_directory</span><span class="o">=</span><span class="s2">&quot;./parallel_ckpt&quot;</span><span class="p">,</span>
    <span class="n">merge_checkpoints</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># Different point in Section 2.6</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="appendix-multi-node-training">
<h2>Appendix. Multi-node Training<a class="headerlink" href="#appendix-multi-node-training" title="Permalink to this heading"></a></h2>
<p>There are three types of training methods are supported by oslo.</p>
<ol class="arabic">
<li><p>torch distributed ( torchrun, recommended )</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Node #1</span>
torchrun --nnodes<span class="o">=</span><span class="m">2</span> --node_rank<span class="o">=</span><span class="m">0</span> --nproc_per_node<span class="o">=</span><span class="m">4</span> --master_addr<span class="o">=</span><span class="si">${</span><span class="nv">YOUR_NODE_ADDRESS</span><span class="si">}</span> --master_port<span class="o">=</span><span class="si">${</span><span class="nv">PORT</span><span class="si">}</span> YOUR_SCRIPT.py

<span class="c1"># Node #2</span>
torchrun --nnodes<span class="o">=</span><span class="m">2</span> --node_rank<span class="o">=</span><span class="m">1</span> --nproc_per_node<span class="o">=</span><span class="m">4</span> --master_addr<span class="o">=</span><span class="si">${</span><span class="nv">YOUR_NODE_ADDRESS</span><span class="si">}</span> --master_port<span class="o">=</span><span class="si">${</span><span class="nv">PORT</span><span class="si">}</span> YOUR_SCRIPT.py
</pre></div>
</div>
</li>
<li><p>Slurm : Slurm using SBATCH file, and then running sbatch sbatch_file.sh command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=${JOBNAME}</span>
<span class="c1">#SBATCH --partition=gpu</span>
<span class="c1">#SBATCH --time=infinite</span>

<span class="c1">### e.g. request 8 nodes with 8 gpu each, totally 64 gpus (WORLD_SIZE==64)</span>
<span class="c1">### Note: --gres=gpu:x should equal to ntasks-per-node</span>
<span class="c1">#SBATCH --nodes=4</span>
<span class="c1">#SBATCH --ntasks-per-node=4</span>
<span class="c1">#SBATCH --cpus-per-task=6</span>
<span class="c1">#SBATCH --gres=gpu:4             # number of gpus per node</span>
<span class="c1">#SBATCH --mem=64gb</span>

<span class="nb">export</span> <span class="nv">HOSTNAMES</span><span class="o">=</span><span class="sb">`</span>scontrol show hostnames <span class="s2">&quot;</span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span><span class="sb">`</span>
<span class="nb">export</span> <span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>scontrol show hostnames <span class="s2">&quot;</span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span> <span class="p">|</span> head -n <span class="m">1</span><span class="k">)</span>
<span class="nb">export</span> <span class="nv">MASTER_PORT</span><span class="o">=</span><span class="si">${</span><span class="nv">PORT</span><span class="si">}</span>
<span class="nb">export</span> <span class="nv">COUNT_NODE</span><span class="o">=</span><span class="sb">`</span>scontrol show hostnames <span class="s2">&quot;</span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span> <span class="p">|</span> wc -l<span class="sb">`</span>

python YOUR_SCRIPT.py
</pre></div>
</div>
<p>And then, run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch sbatch_file.py
</pre></div>
</div>
</li>
</ol>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../CONCEPTS/tensor_model_parallelism.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Concept of Tensor Model Parallelism</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By EleutherAI<br/>
  
      &copy; Copyright 2023, EleutherAI.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>